{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model3-3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Bk9IDMmLV-ZL",
        "colab_type": "code",
        "outputId": "e03d5759-5863-4531-c289-4a3b6bf08602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "!df -h\n",
        "!pip install keras_sequential_ascii\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import print_summary, to_categorical\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
        "from keras_sequential_ascii import sequential_model_to_ascii_printout"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         359G   17G  324G   5% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "tmpfs           6.4G  8.0K  6.4G   1% /var/colab\n",
            "/dev/sda1       365G   21G  345G   6% /opt/bin\n",
            "shm             6.0G  8.0K  6.0G   1% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "Requirement already satisfied: keras_sequential_ascii in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_sequential_ascii) (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.0.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras_sequential_ascii) (1.14.6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1JGHB1bOWOE5",
        "colab_type": "code",
        "outputId": "664e618d-0fdb-4818-cd06-d6c569a9a8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3316
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        " \n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate\n",
        " \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        " \n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        " \n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        " \n",
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        " \n",
        "model.summary()\n",
        " \n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        " \n",
        "#training\n",
        "batch_size = 128\n",
        " \n",
        "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "\n",
        "import datetime\n",
        "starttime = datetime.datetime.now()\n",
        "model5=model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=100,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),\n",
        "                           callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "endtime = datetime.datetime.now()\n",
        "print (endtime - starttime)\n",
        "\n",
        "\n",
        "#save to disk\n",
        "model_json = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights('model.h5') \n",
        " \n",
        "#testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "#print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0])\n",
        "      \n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "\n",
        "plt.figure(0)\n",
        "plt.plot(model5.history['acc'],'mediumaquamarine')\n",
        "plt.plot(model5.history['val_acc'],'royalblue')\n",
        "plt.xticks(np.arange(0, 101, 2.0))\n",
        "plt.rcParams['figure.figsize'] = (8, 6)\n",
        "plt.xlabel(\"Num of Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
        "plt.legend(['train','validation'])\n",
        " \n",
        " \n",
        "plt.figure(1)\n",
        "plt.plot(model5.history['loss'],'mediumaquamarine')\n",
        "plt.plot(model5.history['val_loss'],'royalblue')\n",
        "plt.xticks(np.arange(0, 101, 2.0))\n",
        "plt.rcParams['figure.figsize'] = (8, 6)\n",
        "plt.xlabel(\"Num of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss vs Validation Loss\")\n",
        "plt.legend(['train','validation'])\n",
        " \n",
        " \n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Confusion matrix result\n",
        " \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict(x_test, verbose=2)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        " \n",
        "for ix in range(10):\n",
        "    print(ix, confusion_matrix(np.argmax(y_test,axis=1),y_pred)[ix].sum())\n",
        "cm = confusion_matrix(np.argmax(y_test,axis=1),y_pred)\n",
        "print(cm)\n",
        " \n",
        "# Visualizing of confusion matrix\n",
        "import seaborn as sn\n",
        "import pandas  as pd\n",
        " \n",
        " \n",
        "df_cm = pd.DataFrame(cm, range(10),\n",
        "                  range(10))\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.set(font_scale=1.4)#for label size\n",
        "sn.heatmap(df_cm, cmap=\"BuPu\",annot=True,annot_kws={\"size\": 12})# font size\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 2, 2, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 4,725,290\n",
            "Trainable params: 4,721,322\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "390/390 [==============================] - 118s 303ms/step - loss: 2.2114 - acc: 0.3691 - val_loss: 2.0134 - val_acc: 0.4003\n",
            "Epoch 2/100\n",
            "390/390 [==============================] - 105s 268ms/step - loss: 1.7239 - acc: 0.4975 - val_loss: 1.6686 - val_acc: 0.5139\n",
            "Epoch 3/100\n",
            "390/390 [==============================] - 102s 262ms/step - loss: 1.5350 - acc: 0.5545 - val_loss: 1.4795 - val_acc: 0.5745\n",
            "Epoch 4/100\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 1.4047 - acc: 0.5992 - val_loss: 1.3550 - val_acc: 0.6130\n",
            "Epoch 5/100\n",
            "390/390 [==============================] - 101s 258ms/step - loss: 1.3117 - acc: 0.6354 - val_loss: 3.7317 - val_acc: 0.5402\n",
            "Epoch 6/100\n",
            "390/390 [==============================] - 103s 263ms/step - loss: 1.2553 - acc: 0.6568 - val_loss: 1.3400 - val_acc: 0.6945\n",
            "Epoch 7/100\n",
            "390/390 [==============================] - 102s 262ms/step - loss: 1.1969 - acc: 0.6754 - val_loss: 1.5772 - val_acc: 0.6747\n",
            "Epoch 8/100\n",
            "390/390 [==============================] - 106s 273ms/step - loss: 1.1874 - acc: 0.6866 - val_loss: 1.3163 - val_acc: 0.6383\n",
            "Epoch 9/100\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 1.0783 - acc: 0.7114 - val_loss: 1.1242 - val_acc: 0.6966\n",
            "Epoch 10/100\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 1.0315 - acc: 0.7249 - val_loss: 0.9829 - val_acc: 0.7396\n",
            "Epoch 11/100\n",
            "390/390 [==============================] - 105s 269ms/step - loss: 0.9907 - acc: 0.7397 - val_loss: 0.9486 - val_acc: 0.7537\n",
            "Epoch 12/100\n",
            "390/390 [==============================] - 101s 258ms/step - loss: 0.9545 - acc: 0.7500 - val_loss: 1.0365 - val_acc: 0.7295\n",
            "Epoch 13/100\n",
            "390/390 [==============================] - 104s 268ms/step - loss: 0.9343 - acc: 0.7577 - val_loss: 0.8671 - val_acc: 0.7819\n",
            "Epoch 14/100\n",
            "390/390 [==============================] - 104s 267ms/step - loss: 0.9127 - acc: 0.7677 - val_loss: 0.8507 - val_acc: 0.7860\n",
            "Epoch 15/100\n",
            "390/390 [==============================] - 102s 262ms/step - loss: 0.9050 - acc: 0.7705 - val_loss: 0.8048 - val_acc: 0.8055\n",
            "Epoch 16/100\n",
            "390/390 [==============================] - 104s 267ms/step - loss: 0.8859 - acc: 0.7793 - val_loss: 0.8962 - val_acc: 0.7777\n",
            "Epoch 17/100\n",
            "390/390 [==============================] - 105s 270ms/step - loss: 0.8701 - acc: 0.7830 - val_loss: 0.7826 - val_acc: 0.8144\n",
            "Epoch 18/100\n",
            "390/390 [==============================] - 103s 265ms/step - loss: 0.8527 - acc: 0.7898 - val_loss: 0.8571 - val_acc: 0.7906\n",
            "Epoch 19/100\n",
            "390/390 [==============================] - 104s 268ms/step - loss: 0.8445 - acc: 0.7928 - val_loss: 0.7666 - val_acc: 0.8233\n",
            "Epoch 20/100\n",
            "390/390 [==============================] - 104s 266ms/step - loss: 0.8346 - acc: 0.7978 - val_loss: 0.7774 - val_acc: 0.8193\n",
            "Epoch 21/100\n",
            "390/390 [==============================] - 105s 270ms/step - loss: 0.8240 - acc: 0.8025 - val_loss: 0.8636 - val_acc: 0.7881\n",
            "Epoch 22/100\n",
            "390/390 [==============================] - 113s 290ms/step - loss: 0.8176 - acc: 0.8047 - val_loss: 0.8109 - val_acc: 0.8064\n",
            "Epoch 23/100\n",
            "390/390 [==============================] - 105s 269ms/step - loss: 0.8076 - acc: 0.8081 - val_loss: 0.7591 - val_acc: 0.8251\n",
            "Epoch 24/100\n",
            "390/390 [==============================] - 103s 265ms/step - loss: 0.7999 - acc: 0.8103 - val_loss: 0.7303 - val_acc: 0.8353\n",
            "Epoch 25/100\n",
            "390/390 [==============================] - 104s 267ms/step - loss: 0.7885 - acc: 0.8132 - val_loss: 0.7401 - val_acc: 0.8307\n",
            "Epoch 26/100\n",
            "390/390 [==============================] - 104s 267ms/step - loss: 0.7800 - acc: 0.8177 - val_loss: 0.7199 - val_acc: 0.8411\n",
            "Epoch 27/100\n",
            "390/390 [==============================] - 102s 262ms/step - loss: 0.7806 - acc: 0.8168 - val_loss: 0.7842 - val_acc: 0.8162\n",
            "Epoch 28/100\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 0.7687 - acc: 0.8210 - val_loss: 0.7954 - val_acc: 0.8149\n",
            "Epoch 29/100\n",
            "390/390 [==============================] - 105s 269ms/step - loss: 0.7673 - acc: 0.8227 - val_loss: 0.7408 - val_acc: 0.8348\n",
            "Epoch 30/100\n",
            "390/390 [==============================] - 106s 271ms/step - loss: 0.7570 - acc: 0.8236 - val_loss: 0.7011 - val_acc: 0.8436\n",
            "Epoch 31/100\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 0.7538 - acc: 0.8284 - val_loss: 0.7368 - val_acc: 0.8347\n",
            "Epoch 32/100\n",
            "390/390 [==============================] - 104s 265ms/step - loss: 0.7457 - acc: 0.8302 - val_loss: 0.7680 - val_acc: 0.8280\n",
            "Epoch 33/100\n",
            "390/390 [==============================] - 102s 261ms/step - loss: 0.7385 - acc: 0.8307 - val_loss: 0.6840 - val_acc: 0.8527\n",
            "Epoch 34/100\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 0.7394 - acc: 0.8330 - val_loss: 0.7215 - val_acc: 0.8366\n",
            "Epoch 35/100\n",
            "390/390 [==============================] - 103s 265ms/step - loss: 0.7374 - acc: 0.8321 - val_loss: 0.7023 - val_acc: 0.8474\n",
            "Epoch 36/100\n",
            "390/390 [==============================] - 104s 266ms/step - loss: 0.7360 - acc: 0.8330 - val_loss: 0.7056 - val_acc: 0.8428\n",
            "Epoch 37/100\n",
            "390/390 [==============================] - 104s 266ms/step - loss: 0.7317 - acc: 0.8358 - val_loss: 0.7055 - val_acc: 0.8490\n",
            "Epoch 38/100\n",
            "390/390 [==============================] - 105s 268ms/step - loss: 0.7272 - acc: 0.8363 - val_loss: 0.6894 - val_acc: 0.8522\n",
            "Epoch 39/100\n",
            "390/390 [==============================] - 105s 270ms/step - loss: 0.7233 - acc: 0.8399 - val_loss: 0.7142 - val_acc: 0.8382\n",
            "Epoch 40/100\n",
            "390/390 [==============================] - 105s 270ms/step - loss: 0.7257 - acc: 0.8376 - val_loss: 0.6704 - val_acc: 0.8562\n",
            "Epoch 41/100\n",
            "390/390 [==============================] - 102s 263ms/step - loss: 0.7236 - acc: 0.8389 - val_loss: 0.6629 - val_acc: 0.8551\n",
            "Epoch 42/100\n",
            "390/390 [==============================] - 102s 260ms/step - loss: 0.7177 - acc: 0.8397 - val_loss: 0.6930 - val_acc: 0.8529\n",
            "Epoch 43/100\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 0.7153 - acc: 0.8405 - val_loss: 0.7048 - val_acc: 0.8474\n",
            "Epoch 44/100\n",
            "390/390 [==============================] - 103s 263ms/step - loss: 0.7155 - acc: 0.8418 - val_loss: 0.6690 - val_acc: 0.8565\n",
            "Epoch 45/100\n",
            "390/390 [==============================] - 103s 264ms/step - loss: 0.7054 - acc: 0.8455 - val_loss: 0.6422 - val_acc: 0.8657\n",
            "Epoch 46/100\n",
            "390/390 [==============================] - 112s 287ms/step - loss: 0.7002 - acc: 0.8460 - val_loss: 0.6641 - val_acc: 0.8554\n",
            "Epoch 47/100\n",
            "390/390 [==============================] - 105s 269ms/step - loss: 0.7080 - acc: 0.8445 - val_loss: 0.7145 - val_acc: 0.8450\n",
            "Epoch 48/100\n",
            "390/390 [==============================] - 104s 266ms/step - loss: 0.7007 - acc: 0.8470 - val_loss: 0.6413 - val_acc: 0.8654\n",
            "Epoch 49/100\n",
            " 77/390 [====>.........................] - ETA: 1:14 - loss: 0.6926 - acc: 0.8509"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}